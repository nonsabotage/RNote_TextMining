---
title: "Ch04 単語間の関係:nグラムと相関"
output:
    html_document:
        toc: true
        toc_float: true
        highlight: tango
        theme: flatly
        css: mycss.css
        code_folding: show
        include:
            - in_header: in_head.html
        df_print: "paged"

---


いままでは単位としての単語が
センテンスや文書との間に持つ関係について考えてきた. 
ここでは「ある単語の後に出てきやすい単語」など
同じ文書の中でも共起することが多い単語の間の
関係からも多くの面白い分析が出来ることを示す. 


- tokenにはngramsを使う
- ggraph, widyrで可視化を行う


# ライブラリー

```{r setup, message=FALSE}
config <- yaml::read_yaml("config.yaml")
ap <- available.packages()
for (p in config$LIBS$CRAN) {
  if (! p %in% ap) {
    install.packages(p)
  } 
  library(p, character.only = TRUE)
}
```

# nグラムによるトークン化

単語Xの後に単語Yが続く頻度を調べれば, 
それらの間の関係を示すモデルを作ることができる.
これには`unnest_toknes`でnを指定した上でngramsを
トークンに指定すれば良い. 

> n=2は2つの連続する単語を指しており, バイグラムと呼ばれて
> よく分析に用いられる.

```{r}
austen_bigrams <-
  austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams %>% top_n (20)
```


## nグラムの出現頻度とフィルタリング

```{r}
# countで頻度を算出
austen_bigrams %>%
  count(bigram, sort = TRUE) %>% 
  top_n(20)
```

頻出単語はやはりofやtheといった
**意味があまりないストップワードが目立つ**ことが
わかる. 

そこでストップワードを除く整理を行う. 

```{r}
bigrams_separated <- 
  austen_bigrams %>%
  separate(bigram, into = c("word1", "word2"), sep = " ")

bigrams_filtered <- 
  bigrams_separated %>%
  filter(
    !(word1 %in% stop_words$word |
      word2 %in% stop_words$word
    )
  ) 

# バイグラムの新しい出現頻度リスト
bigram_counts <- 
  bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts %>% top_n(20)
```

```{r}
# filterのために分割したバイグラムを
# uniteで戻す
bigram_united <- 
  bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")

bigram_united %>% top_n(20)
```

トリグラム(n=3)の場合にも同様に処理をする. 

```{r}
austen_books() %>%
  unnest_tokens(
    trigram, text, token = "ngrams", n = 3 
  ) %>%
  separate(trigram, str_c("word", 1:3), sep = " ") %>%
  filter(
    !( select(., matches("word")) %>% 
       reduce(function(x, y) x | (y %in% stop_words$word), .init = FALSE))
  ) %>%
  count(word1, word2, word3, sort = TRUE)
```

## バイグラムの分析

上記のような形式にしておくことで, 
テキストのお予備分析で役に立つ. 

```{r}
# word2がstreet である場合を調べる
bigrams_filtered %>%
  filter(word2 == "street") %>%
  count(book, word1, sort = TRUE)
```

バイグラムを単語として扱えば, `tf-idf`分析が適用できる. 

```{r}
bigram_tf_idf <-
  bigram_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>% top_n(20)
```


```{r fig.height=14, fig.width=9}
bigram_tf_idf %>% 
  arrange(desc(tf_idf)) %>%
  mutate(word = as_factor(bigram) %>% fct_rev()) %>%
  group_by(book) %>% 
  top_n(15, tf_idf) %>% 
  ungroup()  %>%
  ggplot(aes(word, tf_idf, fill = book)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap( ~ book, ncol = 2, scales = "free") + 
  coord_flip() + 
  theme_light() + 
  scale_fill_viridis_d()
```



## センチメント分析のためのバイグラム

ユニグラムのセンチメント分析では, 
単語が独立で出現すると考えていたため, 
単語のコンテキストが無視されている. 
例えば「don't like」でlikeが出現していても
ポジティブな表現であると解釈していた. 
このような例は下記のように多くみられる. 

```{r}
bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE) %>%
  top_n(20)
```

例えば次のようにして, notがついた感情を
表す単語で頻出ものが調べられる. 

```{r}
AFINN <- get_sentiments("afinn")
not_words <-
  bigrams_separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>% 
  count(word2, score, sort = TRUE) %>%
  ungroup()

not_words %>% top_n(20)
```


```{r}
# not で感情表すもののうち間違った方向
not_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = fct_reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) + 
  geom_col(show.legend = TRUE) + 
  xlab("words preceded by \"not\" ") + 
  ylab("Sentiment score * number of occurrences") + 
  coord_flip()

```

「not」以外の否定語の影響を見てみる. 

```{r}
negation_words <- c("not", "no", "never", "without")

negated_words <- 
  bigrams_separated %>%
  filter(word1 %in% negation_words) %>%
  inner_join(AFINN, by = c("word2" = "word")) %>%
  count(word1, word2, score, sort = TRUE) %>%
  ungroup() 

negated_words
```

```{r fig.height=10, fig.width=9}
negated_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  mutate(word2 = as_factor(word2)) %>%
  group_by(word1) %>%
  top_n(15, abs(contribution)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word2, contribution), contribution, fill = contribution > 0)) + 
  geom_col(show.legend = TRUE) + 
  xlab("Sentiment score * # of occurrences ") + 
  ylab("Words preceded by negation term") + 
  coord_flip() + 
  facet_wrap( ~ word1, ncol = 2, scale= "free") + 
  theme_light()
```


## ggraphを使ったネットワークの可視化

単語間の全ての関係を同時に可視化したい. 
次ぎの３つの変数を準備する. 

- from
- to
- weight

```{r}
bigram_graph <- 
  bigram_counts %>%
  filter(n > 20) %>%
  graph_from_data_frame() 

bigram_graph
```


作成したigraphオブジェクトをggraphパッケージを
使って可視化をする. なお,
**グラフを再現したい場合にはset.seedと使うこと.** 

```{r fig.height=9, fig.width=9}
ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link() + 
  geom_node_point() + 
  geom_node_text(
    aes(label = name), vjust = 1, hjust = 1
  ) + 
  theme_light()
```




バイグラムの頻度に合わせて, 線の不透明度を
変更する. 

```{r fig.height=9, fig.width=9}
a <- grid::arrow(
  type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link(
    aes(edge_alpha = n), show.legend = FALSE, 
    arrow = a, 
    end_cap = circle(.7, 'inches')
  ) + 
  geom_node_point(color = "lightblue", size = 5) + 
  geom_node_text(aes(label = name), vjust = 1, hjust  =1) + 
  theme_void()
```


## ほかのテキストのバイグラムを可視化

ここまでの処理を関数にまとめておく. 
**ngrams**が処理出来ているので, 教科書よりも汎用的にかけたのでは?

```{r}
count_ngrams <- function (dataset, n) {
  str_n_words <- str_c("word", seq_len(n))
  sym_n_words <- map(str_n_words, sym)
  dataset %>%
    unnest_tokens(ngram, text, token = "ngrams", n = n) %>%
    separate(ngram, into = str_n_words, sep = " ") %>%
    filter(
      select(., !!! sym_n_words) %>%
      reduce(
        function(x, y) x & !(y %in% stop_words$word), .init = TRUE)) %>%
    count(!!! sym_n_words, sort = TRUE)
}
```


```{r}
visualize_bigrams <- function(bigrams) {
  arr <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") + 
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = arr) +
    geom_node_point(color = "lightblue", size = 5) + 
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) + 
    theme_void()
}
```


```{r fig.height=9, fig.width=9}
kjv <- gutenberg_download(10)

kjv_bigrams <- 
  kjv %>% 
  count_ngrams(2)

# 数字と出現頻度の低い組み合わせを除去
kjv_bigrams %>% 
  filter( 
    n > 40, 
    ! str_detect(word1, "\\d"), 
    ! str_detect(word2, "\\d")) %>%
  visualize_bigrams()

```


# widyrによるバイグラム分析

widyrパッケージでは, 横持ちのデータでの分析を提供してくれる. 
これにより整理データでは見えにくい, 行単位のデータの
比較がやりやすいくなる. 
つまり「データをワイド化して処理を実行してから整理形式に戻す」
というパターンを単純化して提供してくれる. 

## 節単位の出現頻度と相関

```{r}
austen_section_words <- 
  austen_books() %>%
  filter(book == "Pride & Prejudice") %>%
  mutate(section = row_number() %/% 10) %>%
  filter(section > 0) %>%
  unnest_tokens(word, text) %>%
  filter(! word %in% stop_words$word)

austen_section_words %>% head()

```

`widyr::pairwise_count`を使うことで, 便利な集計ができる. 
pairwiseとは, 指定した変数の水準ごとという意味である. 
これにより, 同じ切ないで共起する2つの単語の出現回数を数えることが
できる. 

```{r}
word_pairs <-
  austen_section_words %>%
  pairwise_count(item = word, feature = section, sort = TRUE)

word_pairs %>% top_n(10)
```


## ペアごとの相関


[$\phi$係数](https://en.wikipedia.org/wiki/Phi_coefficient)という
値でペアごとの相関を算出する. 
$n_{01}$は変数1なしで, 変数2が観測された場合である. 



$$
\phi = \frac{n_{11}n_{00}-n_{10}n_{01}}{\sqrt{n_{1・}n_{0・}n_{・0}n_{・1}}}

$$


```{r}
word_cors <- 
  austen_section_words %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, section, sort = TRUE)

word_cors %>% head()
```



```{r}
# 可視化
word_cors %>%
  filter(item1 %in% c ("elizabeth", "pounds", "married", "pride")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap( ~ item1, scales = "free") + 
  coord_flip() + 
  theme_light()
```


ggraphを使えばペア相関についても可視化ができる. 
ただしこのグラフは→の方向は双方向であることに注意する. 
バイグラムは有向グラフであった. 

```{r}
word_cors %>%
  filter(correlation > .15) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") + 
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) + 
  geom_node_point(color = "lightblue", size = 5) + 
  geom_node_text(aes(label = name), repel = TRUE) + 
  theme_void()
```











